# PTP Parallel

This project is based on Alpa (https://github.com/alpa-projects/alpa), which is originally licensed under the Apache License 2.0.

Based on Alpaâ€™s auto-sharding workflow, PTP implements its automatic intra-operator parallelism configurations search in JAX and the XLA compiler. 
It analyzes the IR of the XLA compiler to extract a sequence of ParallelBlocks and uses the JAX frontend for model segment profiling, ultimately finding an efficient intra-operator parallel configuration.

## Environment Setup
**requirement:** 
- Python3.8 
- CUDA11.2 
- nccl2.8.4
- gcc7.5

clone this repo:

    git clone git@github.com:huwf1220/ptp_parallel.git

Install PTP modified alpa python package

    cd ptp_parallel
    pip3 instal -e ".[dev]"

Build and install Jaxlib. The Jaxlib contains Alpa's C++ code modified by PTP.

```
cd build_jaxlib
python3 build/build.py --enable_cuda --dev_install --bazel_options=--override_repository=org_tensorflow=$(pwd)/../third_party/tensorflow-alpa
cd dist
pip3 install -e .
```

## Functionality Check 
**Hardware requirement:** 4 GPUs. 

In this experiment, we run a functionality check by running the GShard MoE and LLAMA2 models on 4 GPUs.
 
### GShard MoE
**Training settings**

```
"MoEModelConfig", [
    "seq_len", "hidden_size", "num_layers", "num_heads", "vocab_size",
    "num_experts", "expert_group_size"
]
MoEModelConfig(1024, 1280, 16, 4, 32000, 32, 2048) micro-batchsize 32
MoEModelConfig(1024, 1280, 16, 4, 32000, 32, 2048) micro-batchsize 64
MoEModelConfig(1024, 1280, 16, 2, 32000, 32, 2048) micro-batchsize 96
MoEModelConfig(1024, 1280, 16, 2, 32000, 32, 2048) micro-batchsize 128
```

You can run all moe functionality check with one script, this will take about 30 minutes.

```
sh ptp_test/scripts/moe_functionality_check.sh
```
We recommend running a single test step by step because PTP's intra-operator parallel configuration search has three passes: analysis, profiling, and search. 
The output of ParallelBlock and the segment profiling status can be quite long. By running it in three passes, you can check the output at each step.

**STEP 1**

```
sh ptp_test/moe/moe_test.sh --num_layer 4 --batch_size 64 --mode analysis 
```
In analysis pass, PTP analyzes data dependencies on the IR generated by the XLA compiler and groups them into several ParallelBlocks, each containing a set of operator primitives. PTP also performs template matching based on ParallelBlocks to extract unique ParallelBlock sequences from the model.

This script will dump the fused ParallelBlocks and the unique ParallelBlock sequence, represented by ParallelBlock indices and Tensor Contraction operator indices.

**Example output:**
```
...
Parallel Block 3:
    47 dot.3 = f32[65536,1280]{1,0} dot(reshape.9, Arg_6.7), lhs_contracting_dims={1}, rhs_contracting_dims={0}
    Plan: 0, 1, 2, 
    2022 dot.87 = f32[1280,1280]{0,1} dot(reshape.9, reshape.237), lhs_contracting_dims={0}, rhs_contracting_dims={1}
    Plan: 0, 2, 1, 
    1946 dot.80 = f32[65536,1280]{1,0} dot(reshape.240, Arg_6.7), lhs_contracting_dims={1}, rhs_contracting_dims={1}
    Plan: 2, 1, 0, 
    45 reshape.9 = f32[65536,1280]{1,0} reshape(transpose.386)
...
ParallelBlocks sequence: (0, 8) mapped to  (0, 8)
ParallelBlocks sequence: (8, 14) mapped to  (8, 14)
ParallelBlocks sequence: (14, 22) mapped to  (0, 8)
ParallelBlocks sequence: (22, 28) mapped to  (8, 14)
global analysis time: 4.587132930755615
```
We recommend further checking other files saved locally from the analysis pass. For example, `ptp_test/moe/debug_analysis_global/strategy_map.txt`, which contains the available parallel strategies for each operator primitive. For the first tensor contraction operator (`Instruction 47 in ParallelBlock3`) mentioned above, there are three available parallel strategies:

```
Instruction 47: %dot.3 = f32[65536,1280]{1,0} dot(f32[65536,1280]{1,0} %reshape.9, f32[1280,1280]{1,0} %Arg_6.7), lhs_contracting_dims={1}, rhs_contracting_dims={0}
Strategy SS = SR x RS @ {0,1}
Strategy SS = SR x RS @ {1,0}
Strategy RS = RS x SS @ {1,0} (allreduce @ 1)
```

**STEP 2**

We run the profile pass after the analysis pass. PTP extracts model segments based on the unique ParallelBlock sequence, builds a sub-search space for each, and profiles the space.

```
sh ptp_test/moe/moe_test.sh --num_layer 4 --batch_size 64 --mode profile
```

**Example output:**
```
NUM SEGMENT is 4

RUN AutoSharding FOR SEGMENT 0 segment_0

segment PB search space: [3, 3, 3, 4]
SegmentIdx: 0 ParallelBlock Strategy: [0, 0, 0, 0] avgcost: 0.8506424427032471 mem_cost(GB): 10.5
SegmentIdx: 0 ParallelBlock Strategy: [0, 0, 0, 1] avgcost: 0.8633566697438558 mem_cost(GB): 11.0
SegmentIdx: 0 ParallelBlock Strategy: [0, 0, 0, 2] avgcost: 1.7111252546310425 mem_cost(GB): 16.5
SegmentIdx: 0 ParallelBlock Strategy: [0, 0, 0, 3] avgcost: 1.0750094254811604 mem_cost(GB): 9.5
...
RUN AutoSharding FOR SEGMENT 1 segment_1

segment PB search space: [3, 3, 3, 3]
SegmentIdx: 1 ParallelBlock Strategy: [0, 0, 0, 0] avgcost: 0.5250330501132542 mem_cost(GB): 12.5
...
```
The sub-search space of a segment depends on the number of fused ParallelBlocks in the segment and the number of selectable parallel strategies for the first tensor contraction operator in each ParallelBlock. 
Depending on the actual training workload, this profiling process will take about 5-10 minutes.

```
overlapped_profiling overall time 411.030641078949
```
Each segment will also generate its own debug dir `debug_segment_x_analysis_segment` and profile result files `*.pkl`.

**STEP 3**

In the final pass, PTP loads the profile information and searches for the optimal parallel strategy for each ParallelBlock in global ParallelBlock sequence.
```
sh ptp_test/moe/moe_test.sh --num_layer 4 --batch_size 64 --mode search
```

This pass takes some time to run because it performs 100 runs on the final searched intra-operator parallel configuration.

**Example output:**

```
searched ParallelBlock Strategies solution: [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
searched training cost: [0.44787622 0.44809792 0.44744757], memory_cost: 20.5
TFLOP_COUNT: 52.9891590144
tflops: [118.31206101529366, 118.2535258476437, 118.42540343857495]
```

We recommend checking the final parallel results in `debug_search`, especially `after_ir.txt`, which contains all operator primitives with their assigned parallel strategies.

### LLAMA2
Same as the MoE model, you can use a script to test LLAMA's analysis, profile, and search passes under multiple settings.

```
sh ptp_test/scripts/llama_functionality_check.sh
```

Alternatively, you can also run a single test step by step.

```
sh ptp_test/llama2/llama_test.sh --num_layer 4 --batch_size 80 --mode analysis
sh ptp_test/llama2/llama_test.sh --num_layer 4 --batch_size 80 --mode profile
sh ptp_test/llama2/llama_test.sh --num_layer 4 --batch_size 80 --mode search
```


## Performance Test
**Hardware requirement:** 8XA100-PCIE 40GB. 

In this experiment, we check our main claimed results on 8 A100-PCIE GPUs.

`ptp_test/scripts` contains test scripts for different platforms, which run training for various model settings and report the throughputs. For example, run:

```
sh ptp_test/scripts/gpt_A100_8GPU.sh
sh ptp_test/scripts/moe_A100_8GPU.sh
```

You can disable the output of the ParallelBlocks and Profiling process by setting `export DUMP_PROFILE_PARALLEL_BLOCK=0`.

**Example output:**
```
========================================================================================================================
============ GPT test: parallel_block_opt:True config:F batchsize:24 layers:2 devices:4 ============
***********Analysis Pass****************
global analysis time: 2.864539384841919
FLOP_COUNT: 59.373627899904
***********Profiling Pass****************
Profiling...
Profiling...
overlapped_profiling overall time 381.45334672927856
***********Search Pass****************
searched ParallelBlock Strategies solution: [1, 1, 1, 1, 1, 1, 1, 1]
searched training cost: [0.29200369 0.29214811 0.29259733], memory_cost: 11.5
TFLOP_COUNT: 59.373627899904
tflops: [203.33177455599028, 203.23125567799693, 202.91924326186674]
=================================================================
========================================================================================================================
============ GPT test: parallel_block_opt:True config:F batchsize:32 layers:2 devices:4 ============
***********Analysis Pass****************
global analysis time: 3.0106263160705566
FLOP_COUNT: 79.164837199872
...
```

The throughputs reported will be further compared with the performance tests of the baseline systems below.

## Baseline Systems Performance Test

### Alpa 

**requirement:** 

same as PTP.

**environment setup**

The `alpa_default` branch in our repository contains the default Alpa and jaxlib libraries for comparison testing. It is based on version `d987466` from the original Alpa repository. 

We made some simple modifications to this default version to support same features in PTP like OutOfMemory error handling, throughput computation, and test codes for default Alpa. 

You can check the difference between this branch with Alpa `d987466` excluding the `alap_test` directory. 

Checkout to `alpa_default` branch.

```
cd ptp_parallel
git checkout -b alpa_default
```

and rebuild jaxlib and reinstall python packeges.
```
pip3 instal -e ".[dev]"
cd build_jaxlib
python3 build/build.py --enable_cuda --dev_install --bazel_options=--override_repository=org_tensorflow=$(pwd)/../third_party/tensorflow-alpa
cd dist
pip3 install -e .
```

**performance test**

`alap_test` directory contains the same performance test scripts as PTP. 

For example, run

```
sh alpa_test/scripts/gpt_A100_8GPU.sh
```
**Example output:**

```
============ Alpa GPT test: config:F batchsize:24 layers:2 devices:8 ============
[0.37685599 0.37689264 0.37616382]
TFLOP_COUNT: 59.373627899904
tflops: [157.54991020463305, 157.5345892757837, 157.83981436363788]

============ Alpa GPT test: config:F batchsize:32 layers:2 devices:8 ============
...
```

### PyTorch

**requirement:** 

- Python 3.10
- CUDA 11.8
- gcc 9.4.0
- gxx 9.4.0
- torch 2.2.1+cu118
- transformers 4.40.0

**environment setup**
```
cd hf_model
pip install requirements.txt
```
**performance test**

```
cd hf_model

./run_bert_node0.sh
./run_gpt_node0.sh
./run_moe_node0.sh
./run_llama_node0.sh 
```

### DeepSpeed-Megatron

**requirement:** 
- Python 3.10
- CUDA 11.8
- gcc 9.4.0
- gxx 9.4.0
- torch 2.2.1+cu118
- transformers 4.40.0
- deepspeed 0.12.0
- apex 0.1

**environment setup**
```
cd Megatron-Deepspeed
pip install requirements.txt
```

**performance test**

```
cd Megatron-DeepSpeed/examples_deepspeed
cd bert_with_pile
./bert.sh
cd MoE
./gpt.sh

cd hf_model
./deepspeed_moe_node0.sh
./deepspeed_llama_node0.sh
```


## Communication Volume and Kernel Time Test
We also recommend comparing the communication volume and communication/computation kernels time.  

`ptp_test/script/enum_and_benchmark` directory contains scripts that report the theoretical communication costs (based on Alpaâ€™s cost model) and actual overheads  for four models running with different **ParallelBlock** strategy combinations.  

For example, run

```
sh ptp_test/script/enum_and_benchmark/gpt_enum_benchmark_comm_comp.sh
```

**Example output:**

```
============ GPT test: parallel_block_opt:True config:F batchsize:16 layers:2 devices:4 ============
generated_search_space: [3, 3, 3, 3, 3, 3, 3, 3]
Comm Volume: 1.63084e+10
Comm Overhead: 0.789642
Comp Overhead: 0.12948
sval [0, 0, 0, 0, 0, 0, 0, 0] [0.9176259  0.91811025 0.91666341] 15588858608
Comm Volume: 1.23822e+10
Comm Overhead: 2.10805
Comp Overhead: 0.138838
sval [0, 0, 0, 1, 0, 0, 0, 1] [2.30844986 2.2975955  2.32280982] 11428905960
Comm Volume: 5.23488e+09
Comm Overhead: 0.333084
Comp Overhead: 0.118085
sval [0, 0, 0, 2, 0, 0, 0, 2] [0.44937861 0.44941962 0.44925261] 9281140192
Comm Volume: 1.60068e+10
...
```

